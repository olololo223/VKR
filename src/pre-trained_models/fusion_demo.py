"""
fusion_demo.py
–î–µ–º–æ: –∫–∞–º–µ—Ä–∞ + –º–∏–∫—Ä–æ—Ñ–æ–Ω -> –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å -> —ç–º–æ—Ü–∏—è

–ü–æ–¥—Å—Ç—Ä–æ–π—Ç–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∏–∂–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
"""
import os
import time
import tempfile
import numpy as np
import sounddevice as sd
import torch
import torch.nn as nn
import torchvision.transforms as T
from PIL import Image
import cv2
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2Model, AutoTokenizer, AutoModel
import whisper

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAMPLE_RATE = 16000
RECORD_SECONDS = 3
CAMERA_INDEX = 0
MODELS_DIR = "models"

RESNET_PTH = os.path.join(MODELS_DIR, "resnet_emotion_light.pth")
WAV2VEC_DIR = os.path.join(MODELS_DIR, "wav2vec2")
RUBERT_DIR = os.path.join(MODELS_DIR, "rubert_emotion_model")
FUSION_PTH = os.path.join(MODELS_DIR, "fusion_model.pth")

EMOTIONS = ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"]

def check_file(path, desc):
    if not os.path.exists(path):
        raise FileNotFoundError(f"{desc} –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

check_file(RESNET_PTH, "ResNet weights")
check_file(WAV2VEC_DIR, "Wav2Vec2 folder")
check_file(RUBERT_DIR, "RuBERT folder")
check_file(FUSION_PTH, "Fusion model weights")

print(f"Device: {DEVICE}")
from torchvision import models

resnet = models.resnet18()
resnet.fc = nn.Identity()
state = torch.load(RESNET_PTH, map_location=DEVICE)
resnet.load_state_dict(state, strict=False)
resnet = resnet.to(DEVICE).eval()

img_transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])

wav2proc = Wav2Vec2Processor.from_pretrained(WAV2VEC_DIR)
wav2model = Wav2Vec2Model.from_pretrained(WAV2VEC_DIR).to(DEVICE).eval()

tokenizer = AutoTokenizer.from_pretrained(RUBERT_DIR)
rubert = AutoModel.from_pretrained(RUBERT_DIR).to(DEVICE).eval()

try:
    whisper_model = whisper.load_model("tiny", device=DEVICE)
except Exception as e:
    print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ whisper:", e)
    whisper_model = None

class FusionModel(nn.Module):
    def __init__(self, dim_img=512, dim_aud=768, dim_txt=768, hidden=512, num_classes=7):
        super().__init__()
        self.img_fc = nn.Linear(dim_img, hidden)
        self.aud_fc = nn.Linear(dim_aud, hidden)
        self.txt_fc = nn.Linear(dim_txt, hidden)
        self.head = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden*3, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    def forward(self, img, aud, txt):
        img = torch.relu(self.img_fc(img))
        aud = torch.relu(self.aud_fc(aud))
        txt = torch.relu(self.txt_fc(txt))
        cat = torch.cat([img, aud, txt], dim=1)
        return self.head(cat)

fusion = FusionModel(dim_img=512, dim_aud=768, dim_txt=768, num_classes=len(EMOTIONS))
fusion.load_state_dict(torch.load(FUSION_PTH, map_location=DEVICE), strict=False)
fusion = fusion.to(DEVICE).eval()

def capture_image_from_cam():
    cap = cv2.VideoCapture(CAMERA_INDEX)
    if not cap.isOpened():
        cap.release()
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É. –ü—Ä–æ–≤–µ—Ä—å –∏–Ω–¥–µ–∫—Å –∫–∞–º–µ—Ä—ã.")
    ret, frame = cap.read()
    cap.release()
    if not ret:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã.")
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil = Image.fromarray(frame_rgb)
    x = img_transform(pil).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        emb = resnet(x)
    return emb

def record_audio(seconds=RECORD_SECONDS, sr=SAMPLE_RATE):
    print(f"‚è∫Ô∏è –ó–∞–ø–∏—Å—å {seconds} —Å–µ–∫—É–Ω–¥...")
    audio = sd.rec(int(seconds * sr), samplerate=sr, channels=1, dtype='float32')
    sd.wait()
    audio = np.squeeze(audio)
    return audio, sr

def audio_to_wav_bytes(audio_np, sr):
    import soundfile as sf
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    sf.write(tmp.name, audio_np, sr)
    tmp.close()
    return tmp.name

def get_audio_embedding_from_np(audio_np, sr):
    inputs = wav2proc(audio_np, sampling_rate=sr, return_tensors="pt", padding=True)
    with torch.no_grad():
        last = wav2model(**inputs.to(DEVICE)).last_hidden_state.mean(dim=1)
    return last

def transcribe_with_whisper(path):
    if whisper_model is None:
        return ""
    try:
        res = whisper_model.transcribe(path, language=None)
        return res.get("text","").strip()
    except Exception as e:
        print("‚ö†Ô∏è Whisper error:", e)
        return ""

def get_text_embedding_from_text(text):
    if not text:
        return torch.zeros((1,768), device=DEVICE)
    inputs = tokenizer(text, truncation=True, padding=True, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        emb = rubert(**inputs).last_hidden_state.mean(dim=1)
    return emb

def pretty_probs(probs):
    lines = []
    for e, p in zip(EMOTIONS, probs):
        lines.append(f"{e:9s} ‚Üí {p*100:5.1f}%")
    return "\n".join(lines)

def run_once():
    print("\nüì∏ –°–Ω–∏–º–µ–º –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã...")
    img_emb = capture_image_from_cam()

    audio_np, sr = record_audio()
    wav_path = audio_to_wav_bytes(audio_np, sr)

    print("üîä –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥ (Wav2Vec2)...")
    try:
        aud_emb = get_audio_embedding_from_np(audio_np, sr)
    except Exception as e:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞—É–¥–∏–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥–∞:", e)
        aud_emb = torch.zeros((1,768), device=DEVICE)

    print("üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —Ä–µ—á—å (Whisper)...")
    text = transcribe_with_whisper(wav_path)
    if text:
        print("üó£  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ:", text)
    else:
        print("‚ÑπÔ∏è  –¢–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω (–ø—É—Å—Ç–æ). –ú–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é.")
        text = input("–í–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç (–∏–ª–∏ enter, —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å): ").strip()

    txt_emb = get_text_embedding_from_text(text)

    with torch.no_grad():
        logits = fusion(img_emb.to(DEVICE), aud_emb.to(DEVICE), txt_emb.to(DEVICE))
        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
        pred_idx = int(np.argmax(probs))
        pred_label = EMOTIONS[pred_idx]

    print("\n" + "="*32)
    print(f"üé≠ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è: {pred_label.upper()}  (–∏–Ω–¥–µ–∫—Å {pred_idx})")
    print(pretty_probs(probs))
    print("="*32 + "\n")

    try:
        os.remove(wav_path)
    except Exception:
        pass

if __name__ == "__main__":
    print("=== Fusion demo ===")
    print("–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ Whisper –∏ –º–æ–¥–µ–ª–∏ HuggingFace –º–æ–≥—É—Ç —Å–∫–∞—á–∏–≤–∞—Ç—å—Å—è (–Ω—É–∂–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ).")
    print("–ù–∞–∂–º–∏ Enter, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é, Ctrl+C —á—Ç–æ–±—ã –≤—ã–π—Ç–∏.")
    try:
        while True:
            input("‚Üí –ù–∞–∂–º–∏ Enter –¥–ª—è –∑–∞–ø–∏—Å–∏ (–∫–∞–º–µ—Ä–∞ + –º–∏–∫—Ä–æ—Ñ–æ–Ω)...")
            run_once()
    except KeyboardInterrupt:
        print("\n–í—ã—Ö–æ–¥. –£–¥–∞—á–∏!")
