"""
fusion_demo.py
–î–µ–º–æ: –∫–∞–º–µ—Ä–∞ + –º–∏–∫—Ä–æ—Ñ–æ–Ω -> –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å -> —ç–º–æ—Ü–∏—è

–ü–æ–¥—Å—Ç—Ä–æ–π—Ç–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∏–∂–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
"""
import os
import time
import tempfile
import numpy as np
import sounddevice as sd
import torch
import torch.nn as nn
import torchvision.transforms as T
from PIL import Image
import cv2
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2Model, AutoTokenizer, AutoModel
import whisper

# ----------------------------
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø–æ–¥–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
# ----------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAMPLE_RATE = 16000
RECORD_SECONDS = 3
CAMERA_INDEX = 0   # –∏–Ω–¥–µ–∫—Å –∫–∞–º–µ—Ä—ã (0 –æ–±—ã—á–Ω–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è / –ø–µ—Ä–≤–∞—è)
MODELS_DIR = "models"

# –ú–µ—Å—Ç–∞ –º–æ–¥–µ–ª–µ–π ‚Äî –ø–æ–º–µ–Ω—è–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
RESNET_PTH = os.path.join(MODELS_DIR, "resnet_emotion_light.pth")
WAV2VEC_DIR = os.path.join(MODELS_DIR, "wav2vec2")  # –ø–∞–ø–∫–∞ —Å config.json + pytorch_model.bin
RUBERT_DIR = os.path.join(MODELS_DIR, "rubert_emotion_model")
FUSION_PTH = os.path.join(MODELS_DIR, "fusion_model.pth")  # –∏–ª–∏ fusion_final.pth

# –ü–æ—Ä—è–¥–æ–∫ —ç–º–æ—Ü–∏–π (–≤–∑—è—Ç–æ –∏–∑ model_config.json —É —Ç–µ–±—è)
EMOTIONS = ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"]

# ----------------------------
# –ü—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–π–ª–æ–≤
# ----------------------------
def check_file(path, desc):
    if not os.path.exists(path):
        raise FileNotFoundError(f"{desc} –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

check_file(RESNET_PTH, "ResNet weights")
check_file(WAV2VEC_DIR, "Wav2Vec2 folder")
check_file(RUBERT_DIR, "RuBERT folder")
check_file(FUSION_PTH, "Fusion model weights")

print(f"Device: {DEVICE}")
# ----------------------------
# –†–µ—Å–Ω–µ—Ç: –±–µ—Ä–µ–º backbone ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥ 512
# ----------------------------
from torchvision import models

resnet = models.resnet18()
# —Å–¥–µ–ª–∞–µ–º –≤—ã–¥–∞—á—É —Ñ–∏—á: –∑–∞–º–µ–Ω–∏–º fc –Ω–∞ Identity
resnet.fc = nn.Identity()
# –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ (–≤–µ—Å –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å fc.weight/fc.bias; –∏—Å–ø–æ–ª—å–∑—É–µ–º strict=False)
state = torch.load(RESNET_PTH, map_location=DEVICE)
resnet.load_state_dict(state, strict=False)
resnet = resnet.to(DEVICE).eval()

img_transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])

# ----------------------------
# Wav2Vec2 (—ç–º–±–µ–¥–¥–∏–Ω–≥ –∞—É–¥–∏–æ)
# ----------------------------
wav2proc = Wav2Vec2Processor.from_pretrained(WAV2VEC_DIR)
wav2model = Wav2Vec2Model.from_pretrained(WAV2VEC_DIR).to(DEVICE).eval()

# ----------------------------
# RuBERT (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
# ----------------------------
tokenizer = AutoTokenizer.from_pretrained(RUBERT_DIR)
rubert = AutoModel.from_pretrained(RUBERT_DIR).to(DEVICE).eval()

# ----------------------------
# Whisper (STT) - –±–µ—Ä—ë–º –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
# ----------------------------
try:
    whisper_model = whisper.load_model("tiny", device=DEVICE)  # "tiny" ‚Äî –±—ã—Å—Ç—Ä—ã–π; –º–æ–∂–Ω–æ "base"
except Exception as e:
    print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ whisper:", e)
    whisper_model = None

# ----------------------------
# Fusion model (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–µ–º, —á–µ–º –æ–±—É—á–∞–ª–∏)
# –ó–¥–µ—Å—å ‚Äî –ø—Ä–æ—Å—Ç–∞—è MLP, –∫–∞–∫ –≤ train_fusion.py
# ----------------------------
class FusionModel(nn.Module):
    def __init__(self, dim_img=512, dim_aud=768, dim_txt=768, hidden=512, num_classes=7):
        super().__init__()
        self.img_fc = nn.Linear(dim_img, hidden)
        self.aud_fc = nn.Linear(dim_aud, hidden)
        self.txt_fc = nn.Linear(dim_txt, hidden)
        self.head = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden*3, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    def forward(self, img, aud, txt):
        img = torch.relu(self.img_fc(img))
        aud = torch.relu(self.aud_fc(aud))
        txt = torch.relu(self.txt_fc(txt))
        cat = torch.cat([img, aud, txt], dim=1)
        return self.head(cat)

fusion = FusionModel(dim_img=512, dim_aud=768, dim_txt=768, num_classes=len(EMOTIONS))
fusion.load_state_dict(torch.load(FUSION_PTH, map_location=DEVICE), strict=False)
fusion = fusion.to(DEVICE).eval()

# ----------------------------
# –£—Ç–∏–ª–∏—Ç—ã: –∑–∞—Ö–≤–∞—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏, –∑–∞–ø–∏—Å—å –∑–≤—É–∫–∞, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
# ----------------------------
def capture_image_from_cam():
    cap = cv2.VideoCapture(CAMERA_INDEX)
    if not cap.isOpened():
        cap.release()
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É. –ü—Ä–æ–≤–µ—Ä—å –∏–Ω–¥–µ–∫—Å –∫–∞–º–µ—Ä—ã.")
    ret, frame = cap.read()
    cap.release()
    if not ret:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã.")
    # OpenCV BGR -> RGB
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil = Image.fromarray(frame_rgb)
    x = img_transform(pil).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        emb = resnet(x)  # [1,512]
    return emb

def record_audio(seconds=RECORD_SECONDS, sr=SAMPLE_RATE):
    # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ sounddevice
    print(f"‚è∫Ô∏è –ó–∞–ø–∏—Å—å {seconds} —Å–µ–∫—É–Ω–¥...")
    audio = sd.rec(int(seconds * sr), samplerate=sr, channels=1, dtype='float32')
    sd.wait()
    audio = np.squeeze(audio)  # shape (n,)
    return audio, sr

def audio_to_wav_bytes(audio_np, sr):
    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ wav (whisper/torchaudio –º–æ–≥—É—Ç —á–∏—Ç–∞—Ç—å –ø—É—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
    import soundfile as sf
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    sf.write(tmp.name, audio_np, sr)
    tmp.close()
    return tmp.name

def get_audio_embedding_from_np(audio_np, sr):
    # wav2vec expects float array; we create inputs via processor
    inputs = wav2proc(audio_np, sampling_rate=sr, return_tensors="pt", padding=True)
    with torch.no_grad():
        last = wav2model(**inputs.to(DEVICE)).last_hidden_state.mean(dim=1)  # [1,768]
    return last

def transcribe_with_whisper(path):
    if whisper_model is None:
        return ""
    try:
        # whisper returns dict with "text" and "language"
        res = whisper_model.transcribe(path, language=None)  # language autodetect
        return res.get("text","").strip()
    except Exception as e:
        print("‚ö†Ô∏è Whisper error:", e)
        return ""

def get_text_embedding_from_text(text):
    if not text:
        return torch.zeros((1,768), device=DEVICE)
    inputs = tokenizer(text, truncation=True, padding=True, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        emb = rubert(**inputs).last_hidden_state.mean(dim=1)  # [1,768]
    return emb

# ----------------------------
# –ì–ª–∞–≤–Ω–æ–µ: —Ü–∏–∫–ª –¥–µ–º–æ
# ----------------------------
def pretty_probs(probs):
    lines = []
    for e, p in zip(EMOTIONS, probs):
        lines.append(f"{e:9s} ‚Üí {p*100:5.1f}%")
    return "\n".join(lines)

def run_once():
    # 1) —Ñ–æ—Ç–æ
    print("\nüì∏ –°–Ω–∏–º–µ–º –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã...")
    img_emb = capture_image_from_cam()  # [1,512]

    # 2) –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ
    audio_np, sr = record_audio()
    wav_path = audio_to_wav_bytes(audio_np, sr)

    # 3) –∞—É–¥–∏–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥
    print("üîä –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥ (Wav2Vec2)...")
    try:
        aud_emb = get_audio_embedding_from_np(audio_np, sr)  # [1,768]
    except Exception as e:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞—É–¥–∏–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥–∞:", e)
        aud_emb = torch.zeros((1,768), device=DEVICE)

    # 4) —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ Whisper
    print("üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —Ä–µ—á—å (Whisper)...")
    text = transcribe_with_whisper(wav_path)
    if text:
        print("üó£  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ:", text)
    else:
        print("‚ÑπÔ∏è  –¢–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω (–ø—É—Å—Ç–æ). –ú–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é.")
        # –¥–∞—ë–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–≤–æ–¥–∞
        text = input("–í–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç (–∏–ª–∏ enter, —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å): ").strip()

    # 5) —Ç–µ–∫—Å—Ç-—ç–º–±–µ–¥–¥–∏–Ω–≥
    txt_emb = get_text_embedding_from_text(text)

    # 6) –≤–ø–µ—Ä–µ–¥ —á–µ—Ä–µ–∑ fusion
    with torch.no_grad():
        logits = fusion(img_emb.to(DEVICE), aud_emb.to(DEVICE), txt_emb.to(DEVICE))
        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
        pred_idx = int(np.argmax(probs))
        pred_label = EMOTIONS[pred_idx]

    # 7) –≤—ã–≤–æ–¥
    print("\n" + "="*32)
    print(f"üé≠ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è: {pred_label.upper()}  (–∏–Ω–¥–µ–∫—Å {pred_idx})")
    print(pretty_probs(probs))
    print("="*32 + "\n")

    # —á–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π wav
    try:
        os.remove(wav_path)
    except Exception:
        pass

# ----------------------------
# –ó–∞–ø—É—Å–∫
# ----------------------------
if __name__ == "__main__":
    print("=== Fusion demo ===")
    print("–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ Whisper –∏ –º–æ–¥–µ–ª–∏ HuggingFace –º–æ–≥—É—Ç —Å–∫–∞—á–∏–≤–∞—Ç—å—Å—è (–Ω—É–∂–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ).")
    print("–ù–∞–∂–º–∏ Enter, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é, Ctrl+C —á—Ç–æ–±—ã –≤—ã–π—Ç–∏.")
    try:
        while True:
            input("‚Üí –ù–∞–∂–º–∏ Enter –¥–ª—è –∑–∞–ø–∏—Å–∏ (–∫–∞–º–µ—Ä–∞ + –º–∏–∫—Ä–æ—Ñ–æ–Ω)...")
            run_once()
    except KeyboardInterrupt:
        print("\n–í—ã—Ö–æ–¥. –£–¥–∞—á–∏!")
